[
    null,
    {
        "title": "ThomasSimonini/ppo-LunarLander-v2",
        "author": "ThomasSimonini",
        "model_name": "ppo-LunarLander-v2",
        "url": "https://huggingface.co/ThomasSimonini/ppo-LunarLander-v2",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nppo-LunarLander-v2\n\n\n\nlike\n10\n\n\nReinforcement Learning\n\nStable-Baselines3\n\nLunarLander-v2\n\ndeep-reinforcement-learning\n\nEval Results\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t1\n\t\t\t\t\n\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nPPO Agent playing LunarLander-v2\nUsage (with Stable-baselines3)\n\n\n\n\n\n\n\n\n\n\nPPO Agent playing LunarLander-v2\n\n\nThis is a trained model of a PPO agent playing LunarLander-v2\nusing the stable-baselines3 library.\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\nTODO: Add your code\nfrom stable_baselines3 import ...\nfrom huggingface_sb3 import load_from_hub\n\n...\n\n\n\n\nDownloads last month14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\t\t\t\t\tSpace using\n\t\t\t\t\t\tThomasSimonini/ppo-LunarLander-v2\n1\n\ud83c\udfc3\nNeanderthal/LunarLander-v2\n\n\n\n\t\tEvaluation results\n\t\t\n\n\nmean_reward\n\t\t\t\t\t\t\ton LunarLander-v2\nself-reported\n\t\t\t\t\t\t\t\n\n-273.72 +/- 71.58\n\n\n\t\t\t\tView on Papers With Code\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "sb3/a2c-Pendulum-v1",
        "author": "sb3",
        "model_name": "a2c-Pendulum-v1",
        "url": "https://huggingface.co/sb3/a2c-Pendulum-v1",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nsb3\n/\na2c-Pendulum-v1\n\n\n\nlike\n1\n\n\nReinforcement Learning\n\nStable-Baselines3\n\nPendulum-v1\n\ndeep-reinforcement-learning\n\nEval Results\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nA2C Agent playing Pendulum-v1\nUsage (with SB3 RL Zoo)\n\nTraining (with the RL Zoo)\n\nHyperparameters\n\n\n\n\n\n\n\n\n\n\nA2C Agent playing Pendulum-v1\n\n\nThis is a trained model of a A2C agent playing Pendulum-v1\nusing the stable-baselines3 library\nand the RL Zoo.\nThe RL Zoo is a training framework for Stable Baselines3\nreinforcement learning agents,\nwith hyperparameter optimization and pre-trained agents included.\n\n\n\n\n\n\t\tUsage (with SB3 RL Zoo)\n\t\n\nRL Zoo: https://github.com/DLR-RM/rl-baselines3-zoo\nSB3: https://github.com/DLR-RM/stable-baselines3\nSB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n# Download model and save it into the logs/ folder\npython -m rl_zoo3.load_from_hub --algo a2c --env Pendulum-v1 -orga sb3 -f logs/\npython enjoy.py --algo a2c --env Pendulum-v1  -f logs/\n\n\n\n\n\n\n\t\tTraining (with the RL Zoo)\n\t\n\npython train.py --algo a2c --env Pendulum-v1 -f logs/\n# Upload the model and generate video (when possible)\npython -m rl_zoo3.push_to_hub --algo a2c --env Pendulum-v1 -f logs/ -orga sb3\n\n\n\n\n\n\n\t\tHyperparameters\n\t\n\nOrderedDict([('ent_coef', 0.0),\n             ('gae_lambda', 0.9),\n             ('gamma', 0.99),\n             ('learning_rate', 'lin_7e-4'),\n             ('max_grad_norm', 0.5),\n             ('n_envs', 8),\n             ('n_steps', 8),\n             ('n_timesteps', 1000000.0),\n             ('normalize', True),\n             ('normalize_advantage', False),\n             ('policy', 'MlpPolicy'),\n             ('policy_kwargs', 'dict(log_std_init=-2, ortho_init=False)'),\n             ('use_rms_prop', True),\n             ('use_sde', True),\n             ('vf_coef', 0.4),\n             ('normalize_kwargs', {'norm_obs': True, 'norm_reward': False})])\n\n\n\n\nDownloads last month0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\tEvaluation results\n\t\t\n\n\nmean_reward\n\t\t\t\t\t\t\ton Pendulum-v1\nself-reported\n\t\t\t\t\t\t\t\n\n-203.15 +/- 125.77\n\n\n\t\t\t\tView on Papers With Code\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ra-XOr/Unity-Pyramids",
        "author": "ra-XOr",
        "model_name": "Unity-Pyramids",
        "url": "https://huggingface.co/ra-XOr/Unity-Pyramids",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nra-XOr\n/\nUnity-Pyramids\n\n\n\nlike\n1\n\n\nReinforcement Learning\n\nUnity ML-Agents\n\nTensorBoard\n\nONNX\n\nunity-ml-agents\n\ndeep-reinforcement-learning\n\nML-Agents-Pyramids\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\nMetrics\nTraining metrics\n\n\t\t\tCommunity\n\t\t\t1\n\t\t\t\t\n\n\n\n\n\n\n\n\n\t\tUse in ml-agents\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nppo Agent playing Pyramids\nUsage (with ML-Agents)\nResume the training\nWatch your Agent play\n\n\n\n\n\n\n\n\n\n\nppo Agent playing Pyramids\n\n\n  This is a trained model of a ppo agent playing Pyramids using the Unity ML-Agents Library.\n\n\n\n\n\n\t\tUsage (with ML-Agents)\n\t\n\n  The Documentation: https://github.com/huggingface/ml-agents#get-started\n  We wrote a complete tutorial to learn to train your first agent using ML-Agents and publish it to the Hub:\n\n\n\n\n\n\t\tResume the training\n\t\n\nmlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\n\n\n\n\n\n\n\t\tWatch your Agent play\n\t\n\n  You can watch your agent playing directly in your browser:.\n\nGo to https://huggingface.co/spaces/unity/ML-Agents-Pyramids\nStep 1: Write your model_id: ra-XOr/Unity-Pyramids\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play \ud83d\udc40\n\n\n\n\nDownloads last month17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "PKU-Alignment/beaver-7b-v1.0-reward",
        "author": "PKU-Alignment",
        "model_name": "beaver-7b-v1.0-reward",
        "url": "https://huggingface.co/PKU-Alignment/beaver-7b-v1.0-reward",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nPKU-Alignment\n/\nbeaver-7b-v1.0-reward\n\n\n\nlike\n11\n\n\nReinforcement Learning\n\nPyTorch\n\n\n\n\nPKU-Alignment/PKU-SafeRLHF\n\n\n\nEnglish\n\nsafe-rlhf\n\nllama\n\nreinforcement-learning-from-human-feedback\n\nbeaver\n\nsafety\n\nai-safety\n\ndeepspeed\n\nrlhf\n\nalpaca\n\n\n\n\narxiv:\n2302.13971\n\n\n\n\n\n\narxiv:\n2307.04657\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t1\n\t\t\t\t\n\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\n\ud83e\uddab Beaver's Reward Model\nModel Details\n\nModel Sources\n\nHow to Use the Reward Model\n\n\n\n\n\n\n\n\n\n\n\t\t\ud83e\uddab Beaver's Reward Model\n\t\n\n\n\n\n\n\n\t\tModel Details\n\t\n\nThe Beaver reward model is a preference model trained using the PKU-SafeRLHF dataset.\nIt can play a role in the safe RLHF algorithm, helping the Beaver model become more helpful.\n\nDeveloped by: the PKU-Alignment Team.\nModel Type: An auto-regressive language model based on the transformer architecture.\nLicense: Non-commercial license.\nFine-tuned from model: LLaMA, Alpaca.\n\n\n\n\n\n\n\t\tModel Sources\n\t\n\n\nRepository: https://github.com/PKU-Alignment/safe-rlhf\nBeaver: https://huggingface.co/PKU-Alignment/beaver-7b-v1.0\nDataset: https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF\nReward Model: https://huggingface.co/PKU-Alignment/beaver-7b-v1.0-reward\nCost Model: https://huggingface.co/PKU-Alignment/beaver-7b-v1.0-cost\nDataset Paper: https://arxiv.org/abs/2307.04657\nPaper: Coming soon...\n\n\n\n\n\n\n\t\tHow to Use the Reward Model\n\t\n\nfrom transformers import AutoTokenizer\nfrom safe_rlhf.models import AutoModelForScore\n\nmodel = AutoModelForScore.from_pretrained('PKU-Alignment/beaver-7b-v1.0-reward', device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained('PKU-Alignment/beaver-7b-v1.0-reward', use_fast=False)\n\ninput = 'BEGINNING OF CONVERSATION: USER: hello ASSISTANT:Hello! How can I help you today?'\n\ninput_ids = tokenizer(input, return_tensors='pt')\noutput = model(**input_ids)\nprint(output)\n\n# ScoreModelOutput(\n#     scores=tensor([[[-19.6476],\n#         [-20.2238],\n#         [-21.4228],\n#         [-19.2506],\n#         [-20.2728],\n#         [-23.8799],\n#         [-22.6898],\n#         [-21.5825],\n#         [-21.0855],\n#         [-20.2068],\n#         [-23.8296],\n#         [-21.4940],\n#         [-21.9484],\n#         [-13.1220],\n#         [ -6.4499],\n#         [ -8.1982],\n#         [ -7.2492],\n#         [ -9.3377],\n#         [-13.5010],\n#         [-10.4932],\n#         [ -9.7837],\n#         [ -6.4540],\n#         [ -6.0084],\n#         [ -5.8093],\n#         [ -6.6134],\n#         [ -5.8995],\n#         [ -9.1505],\n#         [-11.3254]]], grad_fn=<ToCopyBackward0>),\n#     end_scores=tensor([[-11.3254]], grad_fn=<ToCopyBackward0>)\n# )\n\n\n\n\nDownloads last month459\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\t\t\t\t\tDataset used to train\n\t\t\t\t\t\tPKU-Alignment/beaver-7b-v1.0-reward\n\n\nPKU-Alignment/PKU-SafeRLHF\n\n\n\n\t\t\tViewer\n\t\t\t\u2022 \nUpdated\n\t\t\t\tNov 20, 2023\n\u2022 \n\n\t\t\t\t11.2k\n\t\t\t\u2022 \n\n\t\t\t\t54\n\t\t\t\n\n\n\t\t\t\t\t\tSpaces using\n\t\t\t\t\t\tPKU-Alignment/beaver-7b-v1.0-reward\n2\n\ud83d\udcc8\nShuggu/Ddg\ud83d\udc41\nShuggu/Sdk\n\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "Facepalm0/poca-SoccerTwos",
        "author": "Facepalm0",
        "model_name": "poca-SoccerTwos",
        "url": "https://huggingface.co/Facepalm0/poca-SoccerTwos",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nFacepalm0\n/\npoca-SoccerTwos\n\n\n\nlike\n1\n\n\nReinforcement Learning\n\nUnity ML-Agents\n\nTensorBoard\n\nONNX\n\nSoccerTwos\n\ndeep-reinforcement-learning\n\nML-Agents-SoccerTwos\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\nMetrics\nTraining metrics\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in ml-agents\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\npoca Agent playing SoccerTwos\nUsage (with ML-Agents)\nResume the training\nWatch your Agent play\n\n\n\n\n\n\n\n\n\n\npoca Agent playing SoccerTwos\n\n\n  This is a trained model of a poca agent playing SoccerTwos\n  using the Unity ML-Agents Library.\n\n\n\n\n\n\t\tUsage (with ML-Agents)\n\t\n\n  The Documentation: https://unity-technologies.github.io/ml-agents/ML-Agents-Toolkit-Documentation/\n  We wrote a complete tutorial to learn to train your first agent using ML-Agents and publish it to the Hub:\n\nA short tutorial where you teach Huggy the Dog \ud83d\udc36 to fetch the stick and then play with him directly in your\n  browser: https://huggingface.co/learn/deep-rl-course/unitbonus1/introduction\nA longer tutorial to understand how works ML-Agents:\n  https://huggingface.co/learn/deep-rl-course/unit5/introduction\n\n\n\n\n\n\n\t\tResume the training\n\t\n\nmlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\n\n\n\n\n\n\n\t\tWatch your Agent play\n\t\n\n  You can watch your agent playing directly in your browser\n\nIf the environment is part of ML-Agents official environments, go to https://huggingface.co/unity\nStep 1: Find your model_id: Facepalm0/poca-SoccerTwos\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play \ud83d\udc40\n\n\n\n\nDownloads last month16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "juan9/PPO",
        "author": "juan9",
        "model_name": "PPO",
        "url": "https://huggingface.co/juan9/PPO",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\njuan9\n/\nPPO\n\n\n\nlike\n1\n\n\nReinforcement Learning\n\nStable-Baselines3\n\nLunarLander-v2\n\ndeep-reinforcement-learning\n\nEval Results\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nPPO Agent playing LunarLander-v2\nUsage (with Stable-baselines3)\n\n\n\n\n\n\n\n\n\n\nPPO Agent playing LunarLander-v2\n\n\nThis is a trained model of a PPO agent playing LunarLander-v2\nusing the stable-baselines3 library.\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\nTODO: Add your code\nfrom stable_baselines3 import ...\nfrom huggingface_sb3 import load_from_hub\n\n...\n\n\n\n\nDownloads last month5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\tEvaluation results\n\t\t\n\n\nmean_reward\n\t\t\t\t\t\t\ton LunarLander-v2\nself-reported\n\t\t\t\t\t\t\t\n\n258.91 +/- 19.61\n\n\n\t\t\t\tView on Papers With Code\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/ML-Agents-SnowballFight-1vs1",
        "author": "ThomasSimonini",
        "model_name": "ML-Agents-SnowballFight-1vs1",
        "url": "https://huggingface.co/ThomasSimonini/ML-Agents-SnowballFight-1vs1",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nML-Agents-SnowballFight-1vs1\n\n\n\nlike\n4\n\n\nReinforcement Learning\n\nUnity ML-Agents\n\nONNX\n\ndeep-reinforcement-learning\n\n\n\nLicense: \napache-2.0\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in ml-agents\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nSnowball Fight \u2603\ufe0f, a multi-agent environment for ML-Agents made by Hugging Face\nThe Environment\nObservation Space\nAction Space (Discrete)\nAgent Reward Function (dependant):\nAddendum\n\nHow to use it\nSet-up the environment\nWatch the trained agents\nTrain, the agent\n\nTraining info\nConfig File\n\n\n\n\n\n\n\n\n\n\n\t\tSnowball Fight \u2603\ufe0f, a multi-agent environment for ML-Agents made by Hugging Face\n\t\n\n\nA multi-agent environment using Unity ML-Agents Toolkit where two agents compete in a 1vs1 snowball fight game.\n\ud83d\udc49 You can play  it online at this link.\n\u26a0\ufe0f You need to have some skills in ML-Agents if you want to use it if it's not the case check the documentation\n\n\n\n\n\n\t\tThe Environment\n\t\n\n\nTwo agents compete in a 1 vs 1 snowball fight game.\nThe goal is to hit the opponent team while avoiding the opponent's snowballs \u2744\ufe0f.\n\n\n\n\n\n\n\t\tObservation Space\n\t\n\n\nRay-casts:\n\n10 ray-casts forward distributed over 100 degrees: detecting opponent.\n10 ray-casts forward distributed over 100 degrees: detecting walls, shelter and frontier.\n10 ray-casts forward distributed over 100 degrees: detecting snowballs.\n3 ray-casts backward distributed over 45 degrees: detecting wall and shelter.\n\n\nVector Observations:\n\nBool canShoot (you can only shoot a snowball every 2 seconds).\nFloat currentHealth: normalized [0, 1]\nVector3 vertical speed\nVector3 horizontal speed\nVector3 \"home\" position\n\n\n\n\n\n\n\n\n\t\tAction Space (Discrete)\n\t\n\n\nVector Action space:\nFour branched actions corresponding to forward, backward, sideways movement, rotation, and snowball shoot.\n\n\n\n\n\n\n\n\n\t\tAgent Reward Function (dependant):\n\t\n\n\nIf the team is injured:\n0.1 to the shooter.\n\n\nIf the team is dead:\n(1 - accumulated time penalty): when a snowball hits the\n  opponent, the accumulated time penalty decreases by (1 / MaxStep) every fixed update and is reset to 0 at the beginning of an episode.\n(-1) When a snowball hit our team.\n\n\n\n\n\n\n\n\n\t\tAddendum\n\t\n\n\nThere is no friendly fire, which means that an agent can't shoot himself, or in the future, in a 2vs2 game can't shoot a teammate.\n\n\n\n\n\n\n\t\tHow to use it\n\t\n\n\n\n\n\n\n\t\tSet-up the environment\n\t\n\n\nClone this project git clone https://huggingface.co/ThomasSimonini/ML-Agents-SnowballFight-1vs1\nOpen Unity Hub and create a new 3D Project\nIn the cloned project folder, open .\\ML-Agents-SnowballFight-1vs1\\packages and copy manifest.json and package.lock.json\nPaste these two files in Your Unity Project\\Packages => this will install the required packages.\nDrop the SnowballFight-1vs1 unity package to your Unity Project.\n\n\n\n\n\n\n\t\tWatch the trained agents\n\t\n\n\nIf you want to watch the trained agents, open Assets\\1vs1\\Scenes\\1vs1_v2_Training. place the \\ML-Agents-SnowballFight-1vs1\\saved_model\\SnowballFight1vs1-4999988.onnx into BlueAgent and PurpleAgent Model.\n\n\n\n\n\n\n\t\tTrain, the agent\n\t\n\n\nIf you want to train it again, the scene is Assets\\1vs1\\Scenes\\1vs1_v2_Training.\n\n\n\n\n\n\n\t\tTraining info\n\t\n\n\nSnowballFight1vs1 was trained with 5100000 steps.\nThe final ELO score was 1766.452.\n\n\n\n\n\n\n\t\tConfig File\n\t\n\nbehaviors:   SnowballFight1vs1:     trainer_type: ppo     hyperparameters:       batch_size: 2048       buffer_size: 20480       learning_rate: 0.0003       beta: 0.005       epsilon: 0.2       lambd: 0.95       num_epoch: 3       learning_rate_schedule: constant     network_settings:       normalize: false       hidden_units: 512       num_layers: 2       vis_encode_type: simple     reward_signals:       extrinsic:         gamma: 0.99         strength: 1.0     keep_checkpoints: 40     checkpoint_interval: 200000     max_steps: 50000000     time_horizon: 1000     summary_freq: 50000     self_play:       save_steps: 50000       team_change: 200000       swap_steps: 2000       window: 10       play_against_latest_model_ratio: 0.5       initial_elo: 1200.0 \n\n\n\nDownloads last month6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/demo-hf-CartPole-v1",
        "author": "ThomasSimonini",
        "model_name": "demo-hf-CartPole-v1",
        "url": "https://huggingface.co/ThomasSimonini/demo-hf-CartPole-v1",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\ndemo-hf-CartPole-v1\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nStable-Baselines3\n\nCartPole-v1\n\ndeep-reinforcement-learning\n\nEval Results\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nPPO Agent playing CartPole-v1\nUsage (with Stable-baselines3)\n\n\n\n\n\n\n\n\n\n\nPPO Agent playing CartPole-v1\n\n\nThis is a trained model of a PPO agent playing CartPole-v1\nusing the stable-baselines3 library.\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\nTODO: Add your code\nfrom stable_baselines3 import ...\nfrom huggingface_sb3 import load_from_hub\n\n...\n\n\n\n\nDownloads last month1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\tEvaluation results\n\t\t\n\n\nmean_reward\n\t\t\t\t\t\t\ton CartPole-v1\nself-reported\n\t\t\t\t\t\t\t\n\n236.70 +/- 117.42\n\n\n\t\t\t\tView on Papers With Code\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/mlagents-snowballfight-1vs1-ppo",
        "author": "ThomasSimonini",
        "model_name": "mlagents-snowballfight-1vs1-ppo",
        "url": "https://huggingface.co/ThomasSimonini/mlagents-snowballfight-1vs1-ppo",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nmlagents-snowballfight-1vs1-ppo\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\ndeep-reinforcement-learning\n\nmlagents\n\n\n\nLicense: \napache-2.0\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\nYAML Metadata Error:\n\t\"model-index[0].results\" is required\n\t\n\n\n\n\nmlagents-snowballfight-1vs1-ppo \u2603\ufe0f\n\n\n\n\n\n\n\n\n\n\t\tmlagents-snowballfight-1vs1-ppo \u2603\ufe0f\n\t\n\nThis is a saved model of a PPO 1vs1 agent playing Snowball Fight.\n\n\n\nDownloads last month0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\tEvaluation results\n\t\t\n\nModel card error\nThis model's model-index metadata is invalid:\n\t\t\t\tSchema validation error. \"model-index[0].results\" is required\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/ppo-AntBulletEnv-v0",
        "author": "ThomasSimonini",
        "model_name": "ppo-AntBulletEnv-v0",
        "url": "https://huggingface.co/ThomasSimonini/ppo-AntBulletEnv-v0",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nppo-AntBulletEnv-v0\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nStable-Baselines3\n\ndeep-reinforcement-learning\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nppo-Walker2DBulletEnv-v0\nUsage (with Stable-baselines3)\n\nEvaluation Results\n\n\n\n\n\n\n\n\n\n\n\t\tppo-Walker2DBulletEnv-v0\n\t\n\nThis is a pre-trained model of a PPO agent playing AntBulletEnv-v0 using the stable-baselines3 library.\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\nUsing this model becomes easy when you have stable-baselines3 and huggingface_sb3 installed:\npip install stable-baselines3\npip install huggingface_sb3\n\nThen, you can use the model like this:\n\nimport gym\nimport pybullet_envs\n\nfrom huggingface_sb3 import load_from_hub\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n# Retrieve the model from the hub\n## repo_id =  id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name})\n## filename = name of the model zip file from the repository\nrepo_id = \"ThomasSimonini/ppo-AntBulletEnv-v0\"\ncheckpoint = load_from_hub(repo_id = repo_id, filename=\"ppo-AntBulletEnv-v0.zip\")\nmodel = PPO.load(checkpoint)\n\n# Load the saved statistics\nstats_path = load_from_hub(repo_id = repo_id, filename=\"vec_normalize.pkl\")\n\neval_env = DummyVecEnv([lambda: gym.make(\"AntBulletEnv-v0\")])\neval_env = VecNormalize.load(stats_path, eval_env)\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\n\n\n\n\n\n\t\tEvaluation Results\n\t\n\nMean_reward: 3547.01 +/- 33.32\n\n\n\nDownloads last month2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/ppo-BreakoutNoFrameskip-v4",
        "author": "ThomasSimonini",
        "model_name": "ppo-BreakoutNoFrameskip-v4",
        "url": "https://huggingface.co/ThomasSimonini/ppo-BreakoutNoFrameskip-v4",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nppo-BreakoutNoFrameskip-v4\n\n\n\nlike\n2\n\n\nReinforcement Learning\n\nStable-Baselines3\n\ndeep-reinforcement-learning\n\natari\n\nEval Results\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nPPO Agent playing BreakoutNoFrameskip-v4\nEvaluation Results\n\n\nUsage (with Stable-baselines3)\nTraining Code\n\n\n\n\n\n\n\n\n\n\n\t\tPPO Agent playing BreakoutNoFrameskip-v4\n\t\n\nThis is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library.\nThe training report: https://wandb.ai/simoninithomas/HFxSB3/reports/Atari-HFxSB3-Benchmark--VmlldzoxNjI3NTIy\n\n\n\n\n\n\t\tEvaluation Results\n\t\n\nMean_reward: 339.0\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\n\nYou need to use gym==0.19 since it includes Atari Roms.\nThe Action Space is 6 since we use only possible actions in this game.\n\nWatch your agent interacts :\n# Import the libraries\nimport os \n\nimport gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import VecNormalize\n\nfrom stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack\n\nfrom huggingface_sb3 import load_from_hub, push_to_hub\n\n# Load the model\ncheckpoint = load_from_hub(\"ThomasSimonini/ppo-BreakoutNoFrameskip-v4\", \"ppo-BreakoutNoFrameskip-v4.zip\")\n\n# Because we using 3.7 on Colab and this agent was trained with 3.8 to avoid Pickle errors:\ncustom_objects = {\n            \"learning_rate\": 0.0,\n            \"lr_schedule\": lambda _: 0.0,\n            \"clip_range\": lambda _: 0.0,\n        }\n\nmodel= PPO.load(checkpoint, custom_objects=custom_objects)\n\nenv = make_atari_env('BreakoutNoFrameskip-v4', n_envs=1)\nenv = VecFrameStack(env, n_stack=4)\n\nobs = env.reset()\nwhile True:\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = env.step(action)\n    env.render()\n\n\n\n\n\n\n\t\tTraining Code\n\t\n\nimport wandb\nimport gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack, VecVideoRecorder\nfrom stable_baselines3.common.callbacks import CheckpointCallback\n\nfrom wandb.integration.sb3 import WandbCallback\n\nfrom huggingface_sb3 import load_from_hub, push_to_hub\n\nconfig = {\n    \"env_name\": \"BreakoutNoFrameskip-v4\",\n    \"num_envs\": 8,\n    \"total_timesteps\": int(10e6),\n    \"seed\": 661550378,    \n}\n\nrun = wandb.init(\n    project=\"HFxSB3\",\n    config = config,\n    sync_tensorboard = True,  # Auto-upload sb3's tensorboard metrics\n    monitor_gym = True, # Auto-upload the videos of agents playing the game\n    save_code = True, # Save the code to W&B\n    )\n\n# There already exists an environment generator\n# that will make and wrap atari environments correctly.\n# Here we are also multi-worker training (n_envs=8 => 8 environments)\nenv = make_atari_env(config[\"env_name\"], n_envs=config[\"num_envs\"], seed=config[\"seed\"]) #BreakoutNoFrameskip-v4\n\nprint(\"ENV ACTION SPACE: \", env.action_space.n)\n\n# Frame-stacking with 4 frames\nenv = VecFrameStack(env, n_stack=4)\n# Video recorder\nenv = VecVideoRecorder(env, \"videos\", record_video_trigger=lambda x: x % 100000 == 0, video_length=2000)\n\nmodel = PPO(policy = \"CnnPolicy\",\n            env = env,\n            batch_size = 256,\n            clip_range = 0.1,\n            ent_coef = 0.01,\n            gae_lambda = 0.9,\n            gamma = 0.99,\n            learning_rate = 2.5e-4,\n            max_grad_norm = 0.5,\n            n_epochs = 4,\n            n_steps = 128,\n            vf_coef = 0.5,\n            tensorboard_log = f\"runs\",\n            verbose=1,\n            )\n    \nmodel.learn(\n    total_timesteps = config[\"total_timesteps\"],\n    callback = [\n        WandbCallback(\n        gradient_save_freq = 1000,\n        model_save_path = f\"models/{run.id}\",\n        ), \n        CheckpointCallback(save_freq=10000, save_path='./breakout',\n                                         name_prefix=config[\"env_name\"]),\n        ]\n)\n\nmodel.save(\"ppo-BreakoutNoFrameskip-v4.zip\")\npush_to_hub(repo_id=\"ThomasSimonini/ppo-BreakoutNoFrameskip-v4\", \n    filename=\"ppo-BreakoutNoFrameskip-v4.zip\",\n    commit_message=\"Added Breakout trained agent\")\n\n\n\n\nDownloads last month12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\tEvaluation results\n\t\t\n\n\nmean_reward\n\t\t\t\t\t\t\ton BreakoutNoFrameskip-v4\nself-reported\n\t\t\t\t\t\t\t\n\n339.000\n\n\n\t\t\t\tView on Papers With Code\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/ppo-PongNoFrameskip-v4",
        "author": "ThomasSimonini",
        "model_name": "ppo-PongNoFrameskip-v4",
        "url": "https://huggingface.co/ThomasSimonini/ppo-PongNoFrameskip-v4",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nppo-PongNoFrameskip-v4\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nStable-Baselines3\n\ndeep-reinforcement-learning\n\natari\n\nEval Results\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nPPO Agent playing PongNoFrameskip-v4\nEvaluation Results\n\n\nUsage (with Stable-baselines3)\nTraining Code\n\n\n\n\n\n\n\n\n\n\n\t\tPPO Agent playing PongNoFrameskip-v4\n\t\n\nThis is a trained model of a PPO agent playing PongNoFrameskip-v4 using the stable-baselines3 library (our agent is the \ud83d\udfe2 one).\nThe training report: https://wandb.ai/simoninithomas/HFxSB3/reports/Atari-HFxSB3-Benchmark--VmlldzoxNjI3NTIy\n\n\n\n\n\n\t\tEvaluation Results\n\t\n\nMean_reward: 21.00 +/- 0.0\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\n\nYou need to use gym==0.19 since it includes Atari Roms.\nThe Action Space is 6 since we use only possible actions in this game.\n\nWatch your agent interacts :\n# Import the libraries\nimport os \n\nimport gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import VecNormalize\n\nfrom stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack\n\nfrom huggingface_sb3 import load_from_hub, push_to_hub\n\n# Load the model\ncheckpoint = load_from_hub(\"ThomasSimonini/ppo-PongNoFrameskip-v4\", \"ppo-PongNoFrameskip-v4.zip\")\n\n# Because we using 3.7 on Colab and this agent was trained with 3.8 to avoid Pickle errors:\ncustom_objects = {\n            \"learning_rate\": 0.0,\n            \"lr_schedule\": lambda _: 0.0,\n            \"clip_range\": lambda _: 0.0,\n        }\n\nmodel= PPO.load(checkpoint, custom_objects=custom_objects)\n\nenv = make_atari_env('PongNoFrameskip-v4', n_envs=1)\nenv = VecFrameStack(env, n_stack=4)\n\nobs = env.reset()\nwhile True:\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = env.step(action)\n    env.render()\n\n\n\n\n\n\n\t\tTraining Code\n\t\n\nimport wandb\nimport gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack, VecVideoRecorder\nfrom stable_baselines3.common.callbacks import CheckpointCallback\n\nfrom wandb.integration.sb3 import WandbCallback\n\nfrom huggingface_sb3 import load_from_hub, push_to_hub\n\nconfig = {\n    \"env_name\": \"PongNoFrameskip-v4\",\n    \"num_envs\": 8,\n    \"total_timesteps\": int(10e6),\n    \"seed\": 4089164106,    \n}\n\nrun = wandb.init(\n    project=\"HFxSB3\",\n    config = config,\n    sync_tensorboard = True,  # Auto-upload sb3's tensorboard metrics\n    monitor_gym = True, # Auto-upload the videos of agents playing the game\n    save_code = True, # Save the code to W&B\n    )\n\n# There already exists an environment generator\n# that will make and wrap atari environments correctly.\n# Here we are also multi-worker training (n_envs=8 => 8 environments)\nenv = make_atari_env(config[\"env_name\"], n_envs=config[\"num_envs\"], seed=config[\"seed\"]) #PongNoFrameskip-v4\n\nprint(\"ENV ACTION SPACE: \", env.action_space.n)\n\n# Frame-stacking with 4 frames\nenv = VecFrameStack(env, n_stack=4)\n# Video recorder\nenv = VecVideoRecorder(env, \"videos\", record_video_trigger=lambda x: x % 100000 == 0, video_length=2000)\n\n# https://github.com/DLR-RM/rl-trained-agents/blob/10a9c31e806820d59b20d8b85ca67090338ea912/ppo/PongNoFrameskip-v4_1/PongNoFrameskip-v4/config.yml\nmodel = PPO(policy = \"CnnPolicy\",\n            env = env,\n            batch_size = 256,\n            clip_range = 0.1,\n            ent_coef = 0.01,\n            gae_lambda = 0.9,\n            gamma = 0.99,\n            learning_rate = 2.5e-4,\n            max_grad_norm = 0.5,\n            n_epochs = 4,\n            n_steps = 128,\n            vf_coef = 0.5,\n            tensorboard_log = f\"runs\",\n            verbose=1,\n            )\n    \nmodel.learn(\n    total_timesteps = config[\"total_timesteps\"],\n    callback = [\n        WandbCallback(\n        gradient_save_freq = 1000,\n        model_save_path = f\"models/{run.id}\",\n        ), \n        CheckpointCallback(save_freq=10000, save_path='./pong',\n                                         name_prefix=config[\"env_name\"]),\n        ]\n)\n\nmodel.save(\"ppo-PongNoFrameskip-v4.zip\")\npush_to_hub(repo_id=\"ThomasSimonini/ppo-PongNoFrameskip-v4\", \n    filename=\"ppo-PongNoFrameskip-v4.zip\",\n    commit_message=\"Added Pong trained agent\")\n\n\n\n\nDownloads last month11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\tEvaluation results\n\t\t\n\n\nmean_reward\n\t\t\t\t\t\t\ton PongNoFrameskip-v4\nself-reported\n\t\t\t\t\t\t\t\n\n21.000\n\n\n\t\t\t\tView on Papers With Code\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/ppo-QbertNoFrameskip-v4",
        "author": "ThomasSimonini",
        "model_name": "ppo-QbertNoFrameskip-v4",
        "url": "https://huggingface.co/ThomasSimonini/ppo-QbertNoFrameskip-v4",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nppo-QbertNoFrameskip-v4\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nStable-Baselines3\n\ndeep-reinforcement-learning\n\natari\n\nEval Results\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nPPO Agent playing QbertNoFrameskip-v4\nEvaluation Results\n\n\nUsage (with Stable-baselines3)\nTraining Code\n\n\n\n\n\n\n\n\n\n\n\t\tPPO Agent playing QbertNoFrameskip-v4\n\t\n\nThis is a trained model of a PPO agent playing QbertNoFrameskip-v4 using the stable-baselines3 library.\nThe training report: https://wandb.ai/simoninithomas/HFxSB3/reports/Atari-HFxSB3-Benchmark--VmlldzoxNjI3NTIy\n\n\n\n\n\n\t\tEvaluation Results\n\t\n\nMean_reward: 15685.00 +/- 115.217\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\n\nYou need to use gym==0.19 since it includes Atari Roms.\nThe Action Space is 6 since we use only possible actions in this game.\n\nWatch your agent interacts :\n# Import the libraries\nimport os \n\nimport gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import VecNormalize\n\nfrom stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack\n\nfrom huggingface_sb3 import load_from_hub, push_to_hub\n\n# Load the model\ncheckpoint = load_from_hub(\"ThomasSimonini/ppo-QbertNoFrameskip-v4\", \"ppo-QbertNoFrameskip-v4.zip\")\n\n# Because we using 3.7 on Colab and this agent was trained with 3.8 to avoid Pickle errors:\ncustom_objects = {\n            \"learning_rate\": 0.0,\n            \"lr_schedule\": lambda _: 0.0,\n            \"clip_range\": lambda _: 0.0,\n        }\n\nmodel= PPO.load(checkpoint, custom_objects=custom_objects)\n\nenv = make_atari_env('QbertNoFrameskip-v4', n_envs=1)\nenv = VecFrameStack(env, n_stack=4)\n\nobs = env.reset()\nwhile True:\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = env.step(action)\n    env.render()\n\n\n\n\n\n\n\t\tTraining Code\n\t\n\nimport wandb\nimport gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack, VecVideoRecorder\nfrom stable_baselines3.common.callbacks import CheckpointCallback\n\nfrom wandb.integration.sb3 import WandbCallback\n\nfrom huggingface_sb3 import load_from_hub, push_to_hub\n\nconfig = {\n    \"env_name\": \"QbertNoFrameskip-v4\",\n    \"num_envs\": 8,\n    \"total_timesteps\": int(10e6),\n    \"seed\": 1194709219,    \n}\n\nrun = wandb.init(\n    project=\"HFxSB3\",\n    config = config,\n    sync_tensorboard = True,  # Auto-upload sb3's tensorboard metrics\n    monitor_gym = True, # Auto-upload the videos of agents playing the game\n    save_code = True, # Save the code to W&B\n    )\n\n# There already exists an environment generator\n# that will make and wrap atari environments correctly.\n# Here we are also multi-worker training (n_envs=8 => 8 environments)\nenv = make_atari_env(config[\"env_name\"], n_envs=config[\"num_envs\"], seed=config[\"seed\"]) #QbertNoFrameskip-v4\n\nprint(\"ENV ACTION SPACE: \", env.action_space.n)\n\n# Frame-stacking with 4 frames\nenv = VecFrameStack(env, n_stack=4)\n# Video recorder\nenv = VecVideoRecorder(env, \"videos\", record_video_trigger=lambda x: x % 100000 == 0, video_length=2000)\n\nmodel = PPO(policy = \"CnnPolicy\",\n            env = env,\n            batch_size = 256,\n            clip_range = 0.1,\n            ent_coef = 0.01,\n            gae_lambda = 0.9,\n            gamma = 0.99,\n            learning_rate = 2.5e-4,\n            max_grad_norm = 0.5,\n            n_epochs = 4,\n            n_steps = 128,\n            vf_coef = 0.5,\n            tensorboard_log = f\"runs\",\n            verbose=1,\n            )\n    \nmodel.learn(\n    total_timesteps = config[\"total_timesteps\"],\n    callback = [\n        WandbCallback(\n        gradient_save_freq = 1000,\n        model_save_path = f\"models/{run.id}\",\n        ), \n        CheckpointCallback(save_freq=10000, save_path='./qbert',\n                                         name_prefix=config[\"env_name\"]),\n        ]\n)\n\nmodel.save(\"ppo-QbertNoFrameskip-v4.zip\")\npush_to_hub(repo_id=\"ThomasSimonini/ppo-QbertNoFrameskip-v4\", \n    filename=\"ppo-QbertNoFrameskip-v4.zip\",\n    commit_message=\"Added Qbert trained agent\")\n\n\n\n\nDownloads last month3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\t\t\t\t\tSpaces using\n\t\t\t\t\t\tThomasSimonini/ppo-QbertNoFrameskip-v4\n2\n\ud83d\udc40\nThomasSimonini/Compare-Reinforcement-Learning-Agents\ud83c\udfe2\nThomasSimonini/SB3_Atari\n\n\n\n\t\tEvaluation results\n\t\t\n\n\nmean_reward\n\t\t\t\t\t\t\ton QbertNoFrameskip-v4\nself-reported\n\t\t\t\t\t\t\t\n\n15685.00 +/- 115.217\n\n\n\t\t\t\tView on Papers With Code\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/ppo-SeaquestNoFrameskip-v4",
        "author": "ThomasSimonini",
        "model_name": "ppo-SeaquestNoFrameskip-v4",
        "url": "https://huggingface.co/ThomasSimonini/ppo-SeaquestNoFrameskip-v4",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nppo-SeaquestNoFrameskip-v4\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nStable-Baselines3\n\ndeep-reinforcement-learning\n\natari\n\nEval Results\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nPPO Agent playing SeaquestNoFrameskip-v4\nEvaluation Results\n\n\nUsage (with Stable-baselines3)\nTraining Code\n\n\n\n\n\n\n\n\n\n\n\t\tPPO Agent playing SeaquestNoFrameskip-v4\n\t\n\nThis is a trained model of a PPO agent playing SeaquestNoFrameskip-v4 using the stable-baselines3 library.\nThe training report: https://wandb.ai/simoninithomas/HFxSB3/reports/Atari-HFxSB3-Benchmark--VmlldzoxNjI3NTIy\n\n\n\n\n\n\t\tEvaluation Results\n\t\n\nMean_reward: 1820.00 +/- 20.0\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\n\nYou need to use gym==0.19 since it includes Atari Roms.\nThe Action Space is 6 since we use only possible actions in this game.\n\nWatch your agent interacts :\n# Import the libraries\nimport os \n\nimport gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import VecNormalize\n\nfrom stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack\n\nfrom huggingface_sb3 import load_from_hub, push_to_hub\n\n# Load the model\ncheckpoint = load_from_hub(\"ThomasSimonini/ppo-SeaquestNoFrameskip-v4\", \"ppo-SeaquestNoFrameskip-v4.zip\")\n\n# Because we using 3.7 on Colab and this agent was trained with 3.8 to avoid Pickle errors:\ncustom_objects = {\n            \"learning_rate\": 0.0,\n            \"lr_schedule\": lambda _: 0.0,\n            \"clip_range\": lambda _: 0.0,\n        }\n\nmodel= PPO.load(checkpoint, custom_objects=custom_objects)\n\nenv = make_atari_env('SeaquestNoFrameskip-v4', n_envs=1)\nenv = VecFrameStack(env, n_stack=4)\n\nobs = env.reset()\nwhile True:\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = env.step(action)\n    env.render()\n\n\n\n\n\n\n\t\tTraining Code\n\t\n\nimport wandb\nimport gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack, VecVideoRecorder\nfrom stable_baselines3.common.callbacks import CheckpointCallback\n\nfrom wandb.integration.sb3 import WandbCallback\n\nfrom huggingface_sb3 import load_from_hub, push_to_hub\n\nconfig = {\n    \"env_name\": \"SeaquestNoFrameskip-v4\",\n    \"num_envs\": 8,\n    \"total_timesteps\": int(10e6),\n    \"seed\": 2862830927,    \n}\n\nrun = wandb.init(\n    project=\"HFxSB3\",\n    config = config,\n    sync_tensorboard = True,  # Auto-upload sb3's tensorboard metrics\n    monitor_gym = True, # Auto-upload the videos of agents playing the game\n    save_code = True, # Save the code to W&B\n    )\n\n# There already exists an environment generator\n# that will make and wrap atari environments correctly.\n# Here we are also multi-worker training (n_envs=8 => 8 environments)\nenv = make_atari_env(config[\"env_name\"], n_envs=config[\"num_envs\"], seed=config[\"seed\"]) #SeaquestNoFrameskip-v4\n\nprint(\"ENV ACTION SPACE: \", env.action_space.n)\n\n# Frame-stacking with 4 frames\nenv = VecFrameStack(env, n_stack=4)\n# Video recorder\nenv = VecVideoRecorder(env, \"videos\", record_video_trigger=lambda x: x % 100000 == 0, video_length=2000)\n\nmodel = PPO(policy = \"CnnPolicy\",\n            env = env,\n            batch_size = 256,\n            clip_range = 0.1,\n            ent_coef = 0.01,\n            gae_lambda = 0.9,\n            gamma = 0.99,\n            learning_rate = 2.5e-4,\n            max_grad_norm = 0.5,\n            n_epochs = 4,\n            n_steps = 128,\n            vf_coef = 0.5,\n            tensorboard_log = f\"runs\",\n            verbose=1,\n            )\n    \nmodel.learn(\n    total_timesteps = config[\"total_timesteps\"],\n    callback = [\n        WandbCallback(\n        gradient_save_freq = 1000,\n        model_save_path = f\"models/{run.id}\",\n        ), \n        CheckpointCallback(save_freq=10000, save_path='./seaquest',\n                                         name_prefix=config[\"env_name\"]),\n        ]\n)\n\nmodel.save(\"ppo-SeaquestNoFrameskip-v4.zip\")\npush_to_hub(repo_id=\"ThomasSimonini/ppo-SeaquestNoFrameskip-v4\", \n    filename=\"ppo-SeaquestNoFrameskip-v4.zip\",\n    commit_message=\"Added Seaquest trained agent\")\n\n\n\n\nDownloads last month18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\tEvaluation results\n\t\t\n\n\nmean_reward\n\t\t\t\t\t\t\ton SeaquestNoFrameskip-v4\nself-reported\n\t\t\t\t\t\t\t\n\n1820.00 +/- 20.0\n\n\n\t\t\t\tView on Papers With Code\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4",
        "author": "ThomasSimonini",
        "model_name": "ppo-SpaceInvadersNoFrameskip-v4",
        "url": "https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nppo-SpaceInvadersNoFrameskip-v4\n\n\n\nlike\n3\n\n\nReinforcement Learning\n\nStable-Baselines3\n\ndeep-reinforcement-learning\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4\nUsage (with Stable-baselines3)\n\nEvaluation Results\n\n\n\n\n\n\n\n\n\n\n\t\tThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4\n\t\n\nThis is a pre-trained model of a PPO agent playing SpaceInvadersNoFrameskip using the stable-baselines3 library. It is taken from RL-trained-agents\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\nUsing this model becomes easy when you have stable-baselines3 and huggingface_sb3 installed:\npip install stable-baselines3\npip install huggingface_sb3\n\nThen, you can use the model like this:\nimport gym\n\nfrom huggingface_sb3 import load_from_hub\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack\n\n# Retrieve the model from the hub\n## repo_id =  id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name})\n## filename = name of the model zip file from the repository\ncheckpoint = load_from_hub(repo_id=\"ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4\", filename=\"ppo-SpaceInvadersNoFrameskip-v4.zip\")\nmodel = PPO.load(checkpoint)\n\n\n\n\n\n\n\t\tEvaluation Results\n\t\n\nMean_reward: 627.160 (162 eval episodes)\n\n\n\nDownloads last month6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/ppo-Walker2DBulletEnv-v0",
        "author": "ThomasSimonini",
        "model_name": "ppo-Walker2DBulletEnv-v0",
        "url": "https://huggingface.co/ThomasSimonini/ppo-Walker2DBulletEnv-v0",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nppo-Walker2DBulletEnv-v0\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nStable-Baselines3\n\nWalker2DBulletEnv-v0\n\ndeep-reinforcement-learning\n\nEval Results\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nPPO Agent playing Walker2DBulletEnv-v0\nUsage (with Stable-baselines3)\n\n\n\n\n\n\n\n\n\n\nPPO Agent playing Walker2DBulletEnv-v0\n\n\nThis is a trained model of a PPO agent playing Walker2DBulletEnv-v0\nusing the stable-baselines3 library.\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\nTODO: Add your code\nfrom stable_baselines3 import ...\nfrom huggingface_sb3 import load_from_hub\n\n...\n\n\n\n\nDownloads last month0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\tEvaluation results\n\t\t\n\n\nmean_reward\n\t\t\t\t\t\t\ton Walker2DBulletEnv-v0\nself-reported\n\t\t\t\t\t\t\t\n\n29.51 +/- 2.93\n\n\n\t\t\t\tView on Papers With Code\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "ThomasSimonini/stable-baselines3-ppo-LunarLander-v2",
        "author": "ThomasSimonini",
        "model_name": "stable-baselines3-ppo-LunarLander-v2",
        "url": "https://huggingface.co/ThomasSimonini/stable-baselines3-ppo-LunarLander-v2",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nThomasSimonini\n/\nstable-baselines3-ppo-LunarLander-v2\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\ndeep-reinforcement-learning\n\n\n\nLicense: \napache-2.0\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nARCHIVED MODEL, DO NOT USE IT\n\nstable-baselines3-ppo-LunarLander-v2 \ud83d\ude80\ud83d\udc69\u200d\ud83d\ude80\nUse the Model\nInstall the dependencies\nEvaluate the agent\n\nResults\n\n\n\n\n\nmodel-index:\n\nname: stable-baselines3-ppo-LunarLander-v2\n\n\n\n\n\n\n\n\t\tARCHIVED MODEL, DO NOT USE IT\n\t\n\n\n\n\n\n\n\t\tstable-baselines3-ppo-LunarLander-v2 \ud83d\ude80\ud83d\udc69\u200d\ud83d\ude80\n\t\n\nThis is a saved model of a PPO agent playing LunarLander-v2. The model is taken from rl-baselines3-zoo\nThe goal is to correctly land the lander by controlling firing engines (fire left orientation engine, fire main engine and fire right orientation engine).\n\n\ud83d\udc49 You can watch the agent playing by using this notebook\n\n\n\n\n\n\t\tUse the Model\n\t\n\n\n\n\n\n\n\t\tInstall the dependencies\n\t\n\nYou need to use the Stable Baselines 3 Hugging Face version of the library (this version contains the function to load saved models directly from the Hugging Face Hub):\npip install git+https://github.com/simoninithomas/stable-baselines3.git\n\n\n\n\n\n\n\t\tEvaluate the agent\n\t\n\n\u26a0\ufe0fYou need to have Linux or MacOS to be able to use this environment. If it's not the case you can use the colab notebook\n# Import the libraries\nimport gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n# Load the environment\nenv = gym.make('LunarLander-v2')\n\nmodel = PPO.load_from_huggingface(hf_model_id=\"ThomasSimonini/stable-baselines3-ppo-LunarLander-v2\",hf_model_filename=\"LunarLander-v2\")\n \n# Evaluate the agent\neval_env = gym.make('LunarLander-v2')\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n \n# Watch the agent play\nobs = env.reset()\nfor i in range(1000):\n    action, _state = model.predict(obs)\n    obs, reward, done, info = env.step(action)\n    env.render()\n    if done:\n      obs = env.reset()\n\n\n\n\n\n\n\t\tResults\n\t\n\nMean Reward (10 evaluation episodes): 245.63 +/- 10.02\n\n\n\nDownloads last month0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\t\t\t\t\tSpace using\n\t\t\t\t\t\tThomasSimonini/stable-baselines3-ppo-LunarLander-v2\n1\n\ud83d\udd79\ufe0f\nThomasSimonini/Stable-Baselines3\n\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "carlosaguayo/Simonini-ppo-LunarLander-v2",
        "author": "carlosaguayo",
        "model_name": "Simonini-ppo-LunarLander-v2",
        "url": "https://huggingface.co/carlosaguayo/Simonini-ppo-LunarLander-v2",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\ncarlosaguayo\n/\nSimonini-ppo-LunarLander-v2\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nStable-Baselines3\n\ndeep-reinforcement-learning\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nTODO: Fill this model card\n\n\n\n\n\n\n\n\n\n\t\tTODO: Fill this model card\n\t\n\n\n\n\nDownloads last month2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "edbeeching/decision_transformer_atari",
        "author": "edbeeching",
        "model_name": "decision_transformer_atari",
        "url": "https://huggingface.co/edbeeching/decision_transformer_atari",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nedbeeching\n/\ndecision_transformer_atari\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\ndeep-reinforcement-learning\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t2\n\t\t\t\t\n\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\n\nFind here pretrained model weights for the [Decision Transformer] (https://github.com/kzl/decision-transformer).\nWeights are available for 4 Atari games: Breakout, Pong, Qbert and Seaquest. Found in the checkpoints directory.\nWe share models trained for one seed (123), whereas the paper contained weights for 3 random seeds.\n\n\n\n\n\n\t\tUsage\n\t\n\ngit clone https://huggingface.co/edbeeching/decision_transformer_atari\nconda env create -f conda_env.yml\n\nThen, you can use the model like this:\n\nfrom decision_transform_atari import GPTConfig, GPT\n\nvocab_size = 4\nblock_size = 90\nmodel_type = \"reward_conditioned\"\ntimesteps = 2654\n\nmconf = GPTConfig(\n    vocab_size,\n    block_size,\n    n_layer=6,\n    n_head=8,\n    n_embd=128,\n    model_type=model_type,\n    max_timestep=timesteps,\n)\nmodel = GPT(mconf)\n\ncheckpoint_path = \"checkpoints/Breakout_123.pth\"  # or Pong, Qbert, Seaquest\ncheckpoint = torch.load(checkpoint_path)\nmodel.load_state_dict(checkpoint)\n\n\n\n\nDownloads last month0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "mrm8488/a2c-Pong-v0",
        "author": "mrm8488",
        "model_name": "a2c-Pong-v0",
        "url": "https://huggingface.co/mrm8488/a2c-Pong-v0",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nmrm8488\n/\na2c-Pong-v0\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nStable-Baselines3\n\ndeep-reinforcement-learning\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nTODO: Fill this model card\n\n\n\n\n\n\n\n\n\n\t\tTODO: Fill this model card\n\t\n\n\n\n\nDownloads last month2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "mrm8488/a2c-PongNoFrameskip-v0",
        "author": "mrm8488",
        "model_name": "a2c-PongNoFrameskip-v0",
        "url": "https://huggingface.co/mrm8488/a2c-PongNoFrameskip-v0",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nmrm8488\n/\na2c-PongNoFrameskip-v0\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nStable-Baselines3\n\ndeep-reinforcement-learning\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nTODO: Fill this model card\n\n\n\n\n\n\n\n\n\n\t\tTODO: Fill this model card\n\t\n\n\n\n\nDownloads last month2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "osanseviero/ppo-LunarLander-v2",
        "author": "osanseviero",
        "model_name": "ppo-LunarLander-v2",
        "url": "https://huggingface.co/osanseviero/ppo-LunarLander-v2",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nosanseviero\n/\nppo-LunarLander-v2\n\n\n\nlike\n1\n\n\nReinforcement Learning\n\nStable-Baselines3\n\nLunarLander-v2\n\ndeep-reinforcement-learning\n\nEval Results\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nPPO Agent playing LunarLander-v2\nUsage (with Stable-baselines3)\n\n\n\n\n\n\n\n\n\n\nPPO Agent playing LunarLander-v2\n\n\nThis is a trained model of a PPO agent playing LunarLander-v2\nusing the stable-baselines3 library.\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\nTODO: Add your code\nfrom stable_baselines3 import ...\nfrom huggingface_sb3 import load_from_hub\n\n...\n\n\n\n\nDownloads last month1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\tEvaluation results\n\t\t\n\n\nmean_reward\n\t\t\t\t\t\t\ton LunarLander-v2\nself-reported\n\t\t\t\t\t\t\t\n\n-580.22 +/- 0.00\n\n\n\t\t\t\tView on Papers With Code\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "sb3/demo-hf-CartPole-v1",
        "author": "sb3",
        "model_name": "demo-hf-CartPole-v1",
        "url": "https://huggingface.co/sb3/demo-hf-CartPole-v1",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nsb3\n/\ndemo-hf-CartPole-v1\n\n\n\nlike\n1\n\n\nReinforcement Learning\n\nStable-Baselines3\n\ndeep-reinforcement-learning\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\t\tUse in stable-baselines3\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\n\nThis is a pre-trained model of a PPO agent playing CartPole-v1 using the stable-baselines3 library.\n\n\n\n\n\n\t\tUsage (with Stable-baselines3)\n\t\n\nUsing this model becomes easy when you have stable-baselines3 and huggingface_sb3 installed:\npip install stable-baselines3\npip install huggingface_sb3\n\nThen, you can use the model like this:\nimport gym\n\nfrom huggingface_sb3 import load_from_hub\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n# Retrieve the model from the hub\n## repo_id = id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name})\n## filename = name of the model zip file from the repository\ncheckpoint = load_from_hub(\n    repo_id=\"sb3/demo-hf-CartPole-v1\",\n    filename=\"ppo-CartPole-v1\",\n)\nmodel = PPO.load(checkpoint)\n\n# Evaluate the agent and watch it\neval_env = gym.make(\"CartPole-v1\")\nmean_reward, std_reward = evaluate_policy(\n    model, eval_env, render=True, n_eval_episodes=5, deterministic=True, warn=False\n)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n\n\n\n\n\n\n\t\tEvaluation Results\n\t\n\nMean_reward: 500.0\n\n\n\nDownloads last month22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "edbeeching/decision-transformer-gym-halfcheetah-expert",
        "author": "edbeeching",
        "model_name": "decision-transformer-gym-halfcheetah-expert",
        "url": "https://huggingface.co/edbeeching/decision-transformer-gym-halfcheetah-expert",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nedbeeching\n/\ndecision-transformer-gym-halfcheetah-expert\n\n\n\nlike\n1\n\n\nReinforcement Learning\n\nTransformers\n\nPyTorch\n\ndecision_transformer\n\nfeature-extraction\n\ndeep-reinforcement-learning\n\ndecision-transformer\n\ngym-continous-control\n\nInference Endpoints\n\n\n\n\narxiv:\n2106.01345\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\n\n\t\t\tTrain\n\t\t\n\n\n\n\n\n\t\t\tDeploy\n\t\t\n\n\n\n\t\tUse in Transformers\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nDecision Transformer model trained on expert trajectories sampled from the Gym HalfCheetah environment\n\n\n\n\n\n\n\n\n\n\t\tDecision Transformer model trained on expert trajectories sampled from the Gym HalfCheetah environment\n\t\n\nThis is a trained Decision Transformer model trained on expert trajectories sampled from the Gym HalfCheetah environment.\nThe following normlization coeficients are required to use this model:\nmean = [ -0.04489148,  0.03232588,  0.06034835, -0.17081226, -0.19480659, -0.05751596,  0.09701628,  0.03239211, 11.047426,   -0.07997331, -0.32363534,  0.36297753,  0.42322603,  0.40836546,  1.1085187,  -0.4874403,  -0.0737481 ] \nstd = [0.04002118,  0.4107858, 0.54217845,  0.41522816, 0.23796624, 0.62036866, 0.30100912,  0.21737163, 2.2105937, 0.572586, 1.7255033, 11.844218, 12.06324,     7.0495934,  13.499867, 7.195647, 5.0264325]\nSee our Blog Post,  Colab notebook or Example Script for usage.\n\n\n\nDownloads last month48\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "edbeeching/decision-transformer-gym-halfcheetah-medium",
        "author": "edbeeching",
        "model_name": "decision-transformer-gym-halfcheetah-medium",
        "url": "https://huggingface.co/edbeeching/decision-transformer-gym-halfcheetah-medium",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nedbeeching\n/\ndecision-transformer-gym-halfcheetah-medium\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nTransformers\n\nPyTorch\n\ndecision_transformer\n\nfeature-extraction\n\ndeep-reinforcement-learning\n\ndecision-transformer\n\ngym-continous-control\n\nInference Endpoints\n\n\n\n\narxiv:\n2106.01345\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\n\n\t\t\tTrain\n\t\t\n\n\n\n\n\n\t\t\tDeploy\n\t\t\n\n\n\n\t\tUse in Transformers\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nDecision Transformer model trained on medium trajectories sampled from the Gym HalfCheetah environment\n\n\n\n\n\n\n\n\n\n\t\tDecision Transformer model trained on medium trajectories sampled from the Gym HalfCheetah environment\n\t\n\nThis is a trained Decision Transformer model trained on medium trajectories sampled from the Gym HalfCheetah environment.\nThe following normlization coeficients are required to use this model:\nmean = [-0.06845774,  0.01641455, -0.18354906, -0.27624607, -0.34061527, -0.09339716, -0.21321271, -0.08774239,  5.1730075,  -0.04275195, -0.03610836,  0.14053793, 0.06049833,  0.09550975,  0.067391,    0.00562739,  0.01338279] \nstd = [0.07472999,  0.30234998,  0.3020731, 0.34417078, 0.17619242,  0.5072056,  0.25670078,  0.32948127,  1.2574149,   0.7600542,   1.9800916,   6.5653625,  7.4663677,   4.472223, 10.566964, 5.6719327,   7.498259]\nSee our Blog Post,  Colab notebook or Example Script for usage.\n\n\n\nDownloads last month0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "edbeeching/decision-transformer-gym-halfcheetah-medium-replay",
        "author": "edbeeching",
        "model_name": "decision-transformer-gym-halfcheetah-medium-replay",
        "url": "https://huggingface.co/edbeeching/decision-transformer-gym-halfcheetah-medium-replay",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nedbeeching\n/\ndecision-transformer-gym-halfcheetah-medium-replay\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nTransformers\n\nPyTorch\n\ndecision_transformer\n\nfeature-extraction\n\ndeep-reinforcement-learning\n\ndecision-transformer\n\ngym-continous-control\n\nInference Endpoints\n\n\n\n\narxiv:\n2106.01345\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\n\n\t\t\tTrain\n\t\t\n\n\n\n\n\n\t\t\tDeploy\n\t\t\n\n\n\n\t\tUse in Transformers\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nDecision Transformer model trained on medium-replay trajectories sampled from the Gym HalfCheetah environment\n\n\n\n\n\n\n\n\n\n\t\tDecision Transformer model trained on medium-replay trajectories sampled from the Gym HalfCheetah environment\n\t\n\nThis is a trained Decision Transformer model trained on medium-replay trajectories sampled from the Gym HalfCheetah environment.\nThe following normlization coeficients are required to use this model:\nmean = [-0.12880704,  0.37381196, -0.14995988, -0.23479079, -0.28412786, -0.13096535, -0.20157982, -0.06517727,  3.4768248,  -0.02785066, -0.01503525,  0.07697279,  0.01266712,  0.0273253,   0.02316425,  0.01043872, -0.01583941] \nstd = [0.17019016, 1.2844249,  0.33442774, 0.36727592, 0.26092398, 0.4784107, 0.31814206 ,0.33552638, 2.0931616,  0.80374336, 1.9044334,  6.57321, 7.5728636,  5.0697494,  9.105554,   6.0856543,  7.253004,  5]\nSee our Blog Post,  Colab notebook or Example Script for usage.\n\n\n\nDownloads last month2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "edbeeching/decision-transformer-gym-hopper-expert",
        "author": "edbeeching",
        "model_name": "decision-transformer-gym-hopper-expert",
        "url": "https://huggingface.co/edbeeching/decision-transformer-gym-hopper-expert",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nedbeeching\n/\ndecision-transformer-gym-hopper-expert\n\n\n\nlike\n12\n\n\nReinforcement Learning\n\nTransformers\n\nPyTorch\n\ndecision_transformer\n\nfeature-extraction\n\ndeep-reinforcement-learning\n\ndecision-transformer\n\ngym-continous-control\n\nInference Endpoints\n\n\n\n\narxiv:\n2106.01345\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\n\n\t\t\tTrain\n\t\t\n\n\n\n\n\n\t\t\tDeploy\n\t\t\n\n\n\n\t\tUse in Transformers\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nDecision Transformer model trained on expert trajectories sampled from the Gym Hopper environment\n\n\n\n\n\n\n\n\n\n\t\tDecision Transformer model trained on expert trajectories sampled from the Gym Hopper environment\n\t\n\nThis is a trained Decision Transformer model trained on expert trajectories sampled from the Gym Hopper environment.\nThe following normlization coefficients are required to use this model:\nmean = [ 1.3490015,  -0.11208222, -0.5506444,  -0.13188992, -0.00378754,  2.6071432,  0.02322114, -0.01626922, -0.06840388, -0.05183131,  0.04272673]\nstd = [0.15980862, 0.0446214,  0.14307782, 0.17629202, 0.5912333,  0.5899924, 1.5405099,  0.8152689,  2.0173461,  2.4107876,  5.8440027 ]\nSee our Blog Post,  Colab notebook or Example Script for usage.\n\n\n\nDownloads last month463\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\t\t\t\t\tSpaces using\n\t\t\t\t\t\tedbeeching/decision-transformer-gym-hopper-expert\n2\n\ud83d\udc20\nIPN/Test_01_04_22\ud83d\ude80\newave/edbeeching-decision-transformer-gym-hopper-expert\n\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "edbeeching/decision-transformer-gym-hopper-medium",
        "author": "edbeeching",
        "model_name": "decision-transformer-gym-hopper-medium",
        "url": "https://huggingface.co/edbeeching/decision-transformer-gym-hopper-medium",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nedbeeching\n/\ndecision-transformer-gym-hopper-medium\n\n\n\nlike\n2\n\n\nReinforcement Learning\n\nTransformers\n\nPyTorch\n\ndecision_transformer\n\nfeature-extraction\n\ndeep-reinforcement-learning\n\ndecision-transformer\n\ngym-continous-control\n\nInference Endpoints\n\n\n\n\narxiv:\n2106.01345\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\n\n\t\t\tTrain\n\t\t\n\n\n\n\n\n\t\t\tDeploy\n\t\t\n\n\n\n\t\tUse in Transformers\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nDecision Transformer model trained on medium trajectories sampled from the Gym Hopper environment\n\n\n\n\n\n\n\n\n\n\t\tDecision Transformer model trained on medium trajectories sampled from the Gym Hopper environment\n\t\n\nThis is a trained Decision Transformer model trained on medium trajectories sampled from the Gym Hopper environment.\nThe following normlization coefficients are required to use this model:\nmean = [ 1.311279, -0.08469521, -0.5382719,  -0.07201576,  0.04932366,  2.1066856, -0.15017354,  0.00878345, -0.2848186,  -0.18540096, -0.28461286] \nstd = [0.17790751, 0.05444621, 0.21297139, 0.14530419, 0.6124444, 0.85174465, 1.4515252,  0.6751696,  1.536239,   1.6160746,  5.6072536 ]\nSee our Blog Post,  Colab notebook or Example Script for usage.\n\n\n\nDownloads last month628\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\n\t\t\t\t\t\tSpaces using\n\t\t\t\t\t\tedbeeching/decision-transformer-gym-hopper-medium\n2\n\ud83d\udc41\nyumengzhang/edbeeching-decision-transformer-gym-hopper-medium\ud83d\udd25\nshridurga17/edbeeching-decision-transformer-gym-hopper-medium\n\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "edbeeching/decision-transformer-gym-hopper-medium-replay",
        "author": "edbeeching",
        "model_name": "decision-transformer-gym-hopper-medium-replay",
        "url": "https://huggingface.co/edbeeching/decision-transformer-gym-hopper-medium-replay",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nedbeeching\n/\ndecision-transformer-gym-hopper-medium-replay\n\n\n\nlike\n0\n\n\nReinforcement Learning\n\nTransformers\n\nPyTorch\n\ndecision_transformer\n\nfeature-extraction\n\ndeep-reinforcement-learning\n\ndecision-transformer\n\ngym-continous-control\n\nInference Endpoints\n\n\n\n\narxiv:\n2106.01345\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\n\n\t\t\tTrain\n\t\t\n\n\n\n\n\n\t\t\tDeploy\n\t\t\n\n\n\n\t\tUse in Transformers\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nDecision Transformer model trained on medium-replay trajectories sampled from the Gym Hopper environment\n\n\n\n\n\n\n\n\n\n\t\tDecision Transformer model trained on medium-replay trajectories sampled from the Gym Hopper environment\n\t\n\nThis is a trained Decision Transformer model trained on medium-replay trajectories sampled from the Gym Hopper environment.\nThe following normlization coefficients are required to use this model:\nmean = [ 1.2305138,  -0.04371411, -0.44542956, -0.09370098,  0.09094488,  1.3694725, -0.19992675, -0.02286135, -0.5287045,  -0.14465883, -0.19652697]\nstd = [0.17565121, 0.06369286, 0.34383234, 0.19566889, 0.5547985,  1.0510299, 1.1583077,  0.79631287, 1.4802359,  1.6540332,  5.108601]\nSee our Blog Post,  Colab notebook or Example Script for usage.\n\n\n\nDownloads last month0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    },
    {
        "title": "edbeeching/decision-transformer-gym-walker2d-expert",
        "author": "edbeeching",
        "model_name": "decision-transformer-gym-walker2d-expert",
        "url": "https://huggingface.co/edbeeching/decision-transformer-gym-walker2d-expert",
        "describe": "\n\n\nHugging Face\n\n\n\n\n\n\n\n\t\t\t\t\tModels\n\n\t\t\t\t\tDatasets\n\n\t\t\t\t\tSpaces\n\n\t\t\t\t\tPosts\n\n\t\t\t\t\tDocs\n\n\n\n\n\t\t\tSolutions\n\t\t\n\nPricing\n\t\t\t\n\n\n\n\n\n\nLog In\n\t\t\t\t\nSign Up\n\t\t\t\t\t\n\n\n\n\n\nedbeeching\n/\ndecision-transformer-gym-walker2d-expert\n\n\n\nlike\n2\n\n\nReinforcement Learning\n\nTransformers\n\nPyTorch\n\ndecision_transformer\n\nfeature-extraction\n\ndeep-reinforcement-learning\n\ndecision-transformer\n\ngym-continous-control\n\nInference Endpoints\n\n\n\n\narxiv:\n2106.01345\n\n\n\n\n\t\t\tModel card\n\t\t\t\n\t\t\t\n\t\t\nFiles\nFiles and versions\n\n\t\t\tCommunity\n\t\t\t\n\t\t\t\n\t\t\n\n\n\n\n\n\n\n\n\n\t\t\tTrain\n\t\t\n\n\n\n\n\n\t\t\tDeploy\n\t\t\n\n\n\n\t\tUse in Transformers\n\n\n\n\n\t\t\t\t\t\tEdit model card\n\t\t\t\t\t\n\n\n\n\nDecision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment\n\n\n\n\n\n\n\n\n\n\t\tDecision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment\n\t\n\nThis is a trained Decision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment.\nThe following normlization coeficients are required to use this model:\nmean = [ 1.2384834e+00,  1.9578537e-01, -1.0475016e-01, -1.8579608e-01,  2.3003316e-01,  2.2800924e-02, -3.7383768e-01, 3.3779100e-01,  3.9250960e+00, -4.7428459e-03,  2.5267061e-02, -3.9287535e-03, -1.7367510e-02, -4.8212224e-01, 3.5432147e-04, -3.7124525e-03,  2.6285544e-03] \nstd = [0.06664903, 0.16980624, 0.17309439, 0.21843709, 0.74599105, 0.02410989, 0.3729872,  0.6226182,  0.9708009,  0.72936815, 1.504065,   2.495893, 3.511518,   5.3656907, 0.79503316, 4.317483,   6.1784487]\nSee our Blog Post,  Colab notebook or Example Script for usage.\n\n\n\nDownloads last month9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo Preview\n\nReinforcement Learning\n\n\nloading\n\n\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\n\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\n\n\n\n\n"
    }
]